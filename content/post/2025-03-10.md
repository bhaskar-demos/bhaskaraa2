---
author: bhaskar
title: "Language Models in the Era of Big Data and Advanced AI"
date: 2025-03-11
description:
tags:
categories:
---

![AI Learning](/images/ai-eng/outline.png)

In the era of big data and advanced AI, language models have become a critical component of many applications. This article explores the different types of language models and the core components involved in their development.

**Note:** This post is part of ongoing experimental learning and documentation. Content has been collected and compiled from various resources for reference purposes only.

## Language Model Types

### Masked Language Models

Masked Language Models (MLMs) are designed to understand context bidirectionally, meaning they consider both the preceding and following context in a sentence. This capability makes them highly effective for tasks such as sentiment analysis and text classification.

- **Bidirectional Context Understanding:** MLMs can look at words both before and after a given word to understand its context.
- **Used for Sentiment Analysis and Classification:** These models are particularly useful in tasks that require a deep understanding of the overall context, such as determining the sentiment of a text or classifying it into categories.
- **Example:** BERT: One of the most well-known examples of a Masked Language Model is BERT (Bidirectional Encoder Representations from Transformers), which has revolutionized natural language processing tasks.

### Auto-Regressive Models

Auto-Regressive Models predict the next token in a sequence based on the previous tokens. This unidirectional processing makes them suitable for tasks like text generation and translation.

- **Predict Next Token in Sequence:** These models generate text by predicting the next word in a sequence based on the words that have come before it.
- **Unidirectional Processing:** Unlike MLMs, auto-regressive models only consider the context that comes before the current token.
- **Example:** GPT Models: The Generative Pre-trained Transformer (GPT) models, including GPT-3 and GPT-4, are prime examples of auto-regressive models.

## Core Components

### Model Development Responsibilities

Developing a robust language model involves several key responsibilities:

- **Modeling and Training:** Creating the architecture of the model and training it on relevant datasets.
- **Dataset Engineering:** Preparing and preprocessing the data to ensure it is suitable for training the model.
- **Interface Optimization:** Designing user-friendly interfaces for interacting with the model.
- **Continuous Evaluation:** Regularly evaluating the model’s performance to ensure it meets the desired standards.

### Required Skills

Building effective language models requires a blend of traditional machine learning (ML) engineering skills and knowledge of various ML algorithms and neural network architectures.

- **Traditional ML Engineering:** Understanding the fundamentals of machine learning, including data preprocessing, feature engineering, and model evaluation.
- **ML Algorithms:**
  - **Clustering:** Grouping similar data points together.
  - **Logistic Regression:** Predicting categorical outcomes.
  - **Decision Trees:** Making decisions based on a tree of possible outcomes.
  - **Collaborative/Content Filtering:** Recommending items based on user preferences.
- **Neural Network Architectures:**
  - **Feed-forward Networks:** Basic neural networks where connections between nodes do not form a cycle.
  - **Recurrent Networks:** Neural networks designed for sequential data, such as time series or natural language.
  - **Convolutional Networks:** Neural networks particularly effective for image and video recognition.
  - **Transformers:** State-of-the-art models for natural language processing, including both MLMs and auto-regressive models.

### Model Selection Criteria

Choosing the right model involves considering several factors:

- **Build vs. Buy Considerations:**
  - **API Costs vs. Engineering Costs:** Weighing the cost of using pre-built APIs against the cost of developing a model in-house.
  - **Data Privacy Requirements:** Ensuring that the model complies with data privacy regulations.
  - **Latency Requirements:** Meeting the speed requirements for real-time applications.
- **Evaluation Methods:**
  - **Public Benchmarks:** Comparing the model’s performance against established benchmarks.
  - **Task-Specific Evaluation:** Assessing the model’s performance on specific tasks relevant to the application.
  - **Continuous Monitoring:** Regularly monitoring the model’s performance to ensure it remains effective over time.

## Modern AI Development

Modern AI development leverages foundation models and iterative development techniques to create highly effective and specialized AI solutions.

- **Based on Foundation Models:** Utilizing pre-trained models as a starting point for developing more specialized models.
- **Iterative Development Through Prompts:** Refining the model through iterative prompting to improve its performance on specific tasks.
- **Model Optimization Through Fine-Tuning:** Fine-tuning the model on task-specific data to enhance its accuracy and relevance.
- **Integration with RAG (Retrieval-Augmented Generation):** Combining retrieval-based methods with generative models to improve the quality and relevance of generated text.
- **Focus on Specific Use Cases:** Tailoring the model to address specific business or application needs.

## Foundation Models

Foundation models are pre-trained models that have been trained on large datasets and can be used as a starting point for developing more specialized models. Training data quality of these models are critical for their effectiveness.

### Model Dimension Parameters

Model dimensions describe the capacity and complexity of the model. The model’s dimensions are determined by the number of parameters and the size of the model.



| Parameter | Description | Impact | Typical Range |
|-----------|-------------|--------|---------------|
| Hidden Size | - Width of neural network<br>- Dimensions of token representation<br>- Size of internal vectors | - Determines information capacity per token<br>- Affects model's ability to capture complex patterns<br>- Impacts memory usage | - Small: 768-1024<br>- Medium: 2048-4096<br>- Large: 8192+ |
| Layers | - Depth of neural network<br>- Number of sequential transformations<br>- Processing stages | - Deeper understanding<br>- More complex reasoning<br>- Increased computational cost | - Small: 12-24<br>- Medium: 32-40<br>- Large: 60-100+ |
| Attention Heads | - Parallel attention mechanisms<br>- Different aspects of input focus<br>- Usually hidden_size ÷ 64 or 128 | - Multiple perspectives<br>- Better context understanding<br>- Parallel processing capability | - Small: 12-16<br>- Medium: 32-40<br>- Large: 64-96+ |
| Parameters | - Total trainable weights<br>- ~12 × hidden_size² × layers<br>- Memory footprint | - Learning capacity<br>- Resource requirements<br>- Training/inference cost | - Small: <1B<br>- Medium: 1B-70B<br>- Large: >100B |



### Sampling Strategies

Sampling strategies are methods used to generate text based on a model’s output.

# LLM Sampling Strategies

| Sampling Strategy | Description | Pros | Cons | Best Use Case |
|-------------------|-------------|------|------|---------------|
| Greedy (Top-1) | Always selects most likely next token | - Deterministic<br>- Fast<br>- Consistent | - Repetitive<br>- Limited creativity<br>- Can get stuck | - Factual responses<br>- Translation<br>- Short completions |
| Top-K | Samples from K most likely tokens | - Balanced randomness<br>- Controlled diversity<br>- Predictable | - K is hard to tune<br>- May miss good rare tokens | - Creative writing<br>- Dialog<br>- Medium-length text |
| Top-P (Nucleus) | Samples from tokens comprising P cumulative probability | - Dynamic vocabulary size<br>- Natural variation<br>- Context-aware | - Can be unstable<br>- More complex<br>- P needs tuning | - Story generation<br>- Long-form content<br>- Creative tasks |
| Temperature | Adjusts probability distribution sharpness | - Simple to implement<br>- Intuitive control<br>- Flexible | - Global effect<br>- Can be unpredictable | - Adjusting creativity<br>- Controlling randomness<br>- Fine-tuning output |
| Beam Search | Maintains multiple candidate sequences | - High quality output<br>- Considers context<br>- More coherent | - Computationally expensive<br>- Memory intensive<br>- Can be repetitive | - Translation<br>- Summarization<br>- Structured output |


- **Common Settings:**
  - **Temperature:** 0.7-1.0 for creative, 0.2-0.5 for factual
  - **Top-P:** 0.9-0.95 typical
  - **Top-K:** 40-50 typical
  - **Beam width:** 4-10 beams

### Prompt

Input instructions to guide AI systems in generating accurate, relevant, and high-quality responses.

- **Example techniques:**
  - Zero-shot prompting
  - Few-shot prompting
  - Chain-of-thought prompting
  - Role-playing prompts
  - Constraint-based prompting

## Structure

Foundation models serve as the backbone for large language models (LLMs), providing a pre-trained starting point that can be fine-tuned for specific tasks. These models have memory through stored context, allowing them to maintain and utilize relevant information over time. Indexing documents to create context is a crucial component, enabling the retrieval of pertinent data when needed. Supporting tools, such as Langchain and Langindex, further enhance the capabilities of foundation models, facilitating the development of retrieval-augmented generation (RAG) and agentic workflows. These tools help in integrating retrieval mechanisms with generative models, ensuring that the generated outputs are accurate, contextually relevant, and tailored to specific applications.

### Example Prompt

**System:**
- **Role:** Expert Data Analyst and SQL Query Specialist
- **Tools Available:**
  - DataAnalyzer
  - QueryOptimizer
  - PerformanceProfiler
- **Instructions:**
  - Write precise, efficient SQL queries
  - Provide clear explanations
  - Consider business insights
  - Optimize for performance

**Context:**
- **Database:** Beverage Company Retail Management System
- **Key Tables:** stores, products, inventory, sales_history
- **Database Type:** PostgreSQL
- **Reporting Focus:** Sales and Inventory Performance

**User:** Create a comprehensive query to analyze top-performing stores based on total sales volume and inventory efficiency, including:
1. Total sales amount per store
2. Average product stock levels
3. Number of unique products sold
4. Rank stores by their sales performance

**DataAnalyzer:**
- Identified key relationships between stores, sales, and inventory
- Recommended joining sales_history, stores, and inventory tables
- Suggested performance optimizations

**QueryOptimizer:**
- Proposed aggregation strategy
- Recommended indexing on store_id and transaction timestamps

**PerformanceProfiler:**
- Estimated query complexity
- Suggested potential optimization techniques

## Retrieval-Augmented Generation (RAG)

RAG works by first retrieving relevant documents or information from a large corpus based on the input query. Then, it uses this retrieved information to augment the generative model, ensuring that the generated text is both accurate and contextually appropriate.

![Retrieval-Augmented Generation](/images/ai-eng/rag.png)

## Agentic AI and Agentic Workflow

![Agentic AI and Agentic Workflow](/images/ai-eng/agent.png)

Implement RAG or Agentic workflow through Langchain and Langindex.
![Agentic AI and Agentic Workflow](/images/ai-eng/langchain.png)

## Summary

In conclusion, language models are indispensable tools in the realm of big data and advanced AI, with applications spanning from education and image generation to chatbots and more. Understanding the different types of language models, such as Masked Language Models and Auto-Regressive Models, and the core components involved in their development is crucial for leveraging their full potential. By considering factors such as build vs. buy considerations, data privacy requirements, and continuous evaluation, organizations can select and optimize the right models for their specific needs. Modern AI development, which relies on foundation models and iterative development techniques, further enhances the effectiveness and specialization of these models. As AI continues to evolve, the role of language models will only become more prominent, driving innovation and efficiency across various industries.

## References

- [generative-ai-enterprise](https://a16z.com/generative-ai-enterprise-2024/)
- [Super natural instructions](https://aclanthology.org/2022.emnlp-main.340.pdf)
- [Book: AI-Engineering-Building-Applications](https://www.amazon.com/AI-Engineering-Building-Applications-Foundation/dp/1098166302?)
- [The Cost of Reasoning in Raw Intelligence](https://semaphore.substack.com/p/the-cost-of-reasoning-in-raw-intelligence)
- [vLLM](https://arxiv.org/pdf/2309.06180)
- [MMLU Vs Cost](https://semaphore.substack.com/p/the-cost-of-reasoning-in-raw-intelligence)
- [Hugging Face Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)
- [ReAct SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS](https://arxiv.org/pdf/2210.03629)
